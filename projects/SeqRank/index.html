---
layout: default
title: Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback
nav_order: 2309
parent: Home
---


<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback">
  <meta name="keywords" content="Reinforcement Learning from Human Feedback, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id="></script> -->
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-YBLEPL8HN9');
</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    /* Define a CSS class for your section with a specific background color */
    .colored-section {
      background-color: #F8F8F8; /* Replace with your desired color code or name */
      padding: 20px; /* Optional: Add padding for better spacing */
    }

    .colored-section2 {
      background-color: #f3ebde; /* Replace with your desired color code or name */
      padding: 20px; /* Optional: Add padding for better spacing */
    }

    .colored-section3 {
      background-color: #f0eeee; /* Replace with your desired color code or name */
      padding: 20px; /* Optional: Add padding for better spacing */
    }

    ol {
      margin-left: 50px; /* Adjust the value to control the tab size */
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://minyoung1005.github.io/">Minyoung Hwang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://rllab.snu.ac.kr/people/gunmin-lee/gunmin-lee">Gunmin Lee</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://rllab.snu.ac.kr/people/hogun-kee/hogun-kee">Hogun Kee</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/railab/members#h.y40brolczzjt">Chan Woo Kim</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/railab/professor">Kyungjae Lee</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://rllab.snu.ac.kr/people/songhwai-oh">Songhwai Oh</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Robot Learning Lab, Seoul National University, <sup>2</sup>RAI LAB, Chung-Ang University</span>
          </div>
          <div class="is-size-6 publication-venues" style="color: #1ABC9C ">
            <span><b>NeurIPS 2023</b></span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=MIYBTjCVjR"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href=''
                   class="external-link button is-normal is-rounded is-light">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon) </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/overview_label.png" class="interpolation-image" width="800" alt="Interpolate start reference image." />

    </div>
  </div>
</section> -->


<section class="colored-section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement learning from human feedback (RLHF) alleviates the problem of designing a task-specific reward function in reinforcement learning by learning it from human preference. 
            However, existing RLHF models are considered inefficient as they produce only a single preference data from each human feedback. 
            To tackle this problem, we propose a novel RLHF framework called SeqRank, that uses sequential preference ranking to enhance the feedback efficiency.
            Our method samples trajectories in a sequential manner by iteratively selecting a defender from the set of previously chosen trajectories K and a challenger from the set of unchosen trajectories U \ K, where U is the replay buffer. 
            We propose two trajectory comparison methods with different defender sampling strategies: (1) sequential pairwise comparison that selects the most recent trajectory and (2) root pairwise comparison that selects the most preferred trajectory from K.
            We construct a data structure and rank trajectories by preference to augment additional queries. The proposed method results in at least 39.2% higher average feedback efficiency than the baseline and also achieves a balance between feedback efficiency and data dependency. 
            We examine the convergence of the empirical risk and the generalization bound of the reward model with Rademacher complexity.
            While both trajectory comparison methods outperform conventional pairwise comparison, root pairwise comparison improves the average reward in locomotion tasks and the average success rate in manipulation tasks by 29.0% and 25.0%, respectively.
          </p>
          </p>
        </div>
      </div>
    </div>
    <!--/ Overview. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/1SF8_6BsA1c"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
        <video id="publication-video" controls playsinline height="100%">
          <source src="./rnr_map/videos/RNR_supplementary_video.mp4"
                  type="video/mp4">
        </video>
        
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Matting. -->
    <div class="column is-centered">
      <h2 class="title is-3">Reinforcement Learning from Human Feedback (RLHF)</h2>
      
      <p>
        Reinforcement learning from human feedback (RLHF) directly learns from human preferences without the need for a hand-crafted reward function.
      </p>
      <br><br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/images/overview_rlhf_only.png" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 60%;" />
      </div>
      <br>
      <br>

      <p>
        A conventional way to learn a reward function in RLHF is pairwise comparison.
      </p>
      <br><br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/images/overview_2.png" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 80%;" />
      </div>
      <br>

      <p>
        Using pairwise comparison, the agent queries a human to compare two different trajectories.
        The feedback efficiency is also fixed as a standardized level, 1.
      </p>
      <br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/pairwise_comparison.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 100%;" />
      </div>
      <br>

    </div>

    <div class="column is-centered">
      <h2 class="title is-3">SeqRank</h2>

      <p>
        We propose a novel RLHF framework called SeqRank.
        Our method uses sequential preference ranking to enhance the feedback efficiency and reduce human’s labeling effort.
      </p>
      <br><br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/images/transitivity.png" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 70%;" />
      </div>
      <br>
      <br>

      <p>
        The key idea of our approach is to utilize the preference relationships of the previous trajectory pairs.
        Bringing the nature of transitivity in human preferences, we can augment preference data.
      </p>
      <br><br>

      <p>
        Our method samples trajectories in a sequential manner by iteratively selecting
        a defender from the set of previously chosen trajectories K and
        a challenger from the set of unchosen trajectories U \ K.
        Specifically, we propose two trajectory comparison methods with different defender sampling strategies:
        <ol>
          <li>Sequential Pairwise Comparison:
            defender = most recently sampled trajectory</li>
          <li> Root Pairwise Comparison:
            defender = previously most preferred trajectory</li>
        </ol>
        <!-- (1) Sequential Pairwise Comparison
        defender = most recently sampled trajectory
        (2) Root Pairwise Comparison
        defender = previously most preferred trajectory -->
      </p>
      <br>
      <br>

      <h3 class="title is-4 ">Sequential Pairwise Comparison</h3>
      <p>
        Sequential pairwise comparison selects the most recently sampled trajectory as the defender.
      </p>
      <br><br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/sequential_pairwise_comparison.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 100%;" />
      </div>

      <h3 class="title is-4 ">Root Pairwise Comparison</h3>
      <p>
        Root pairwise comparison selects the previously most preferred trajectory as the defender.
      </p>
      <br><br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/root_pairwise_comparison.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 100%;" />
      </div>
      <br>
      
      <p>
        Both sequential and root pairwise comparison can augment additional preference data due to transitivity.
      </p>
      <br><br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/images/trajectory_comparison_methods_medium.png" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 80%;" />
      </div>

      <h3 class="title is-4 ">Toy Example</h3>
      <p>
        Suppose the reward values for segments σ_1, σ_2, …, σ_10 are 2, 5, 1, 8, 6, 4, 3, 7, 9, 10, respectively.
        Then, we can construct a graph for each trajectory comparison method.
      </p>
      <br><br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/images/toy_example.png" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 80%;" />
      </div>
      <br>
      <p>
        Black lines indicate actual pairs that receive true preference labels from human feedback.
        Purple lines describe augmented labels for non-adjacent pairs.
      </p>
      <br><br>

      <h3 class="title is-3 ">Theoretical Analyses</h3>
      <p>
        We prove that sequential and root pairwise comparison show 39.2% and 100% higher average feedback efficiency compared to conventional pairwise comparison.
      <br><br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/images/table-1.png" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 80%;" />
      </div>
      <br>

      <h3 class="title is-4 ">Trade-Off between Feedback Efficiency and Data Dependency</h3>
      <p>
        blahblah
      </p>
      <br><br>

    </div>

    <div class="column is-centered">
      <h2 class="title is-3">Results</h2>
      <h3 class="title is-4 ">Simulation Experiments</h3>
      
      <p>
        We show that the overall performance in DMControl is in the order of root, sequential, and pairwise comparison.
      </p>
      <br><br>

      <h3 class="title is-5 ">DMControl - Quadruped Walk</h3>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/quadruped.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <p>
        In the example trajectories in the quadruped walk task,
        the agent trained using pairwise comparison fails to turn its body upside down.
      </p>
      <br><br>

      <h3 class="title is-5 ">DMControl - Walker Walk</h3>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/walker.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <p>
        In the example trajectories in the walker walk task,
        the agent trained using root pairwise comparison shows the fastest and most stable gait.
      </p>
      <br><br>

      <h3 class="title is-5 ">MetaWorld - Drawer Open - Scenario I</h3>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/drawer_open_1.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <p>
        In the first scenario in the drawer open task,
        the agent trained using pairwise comparison fails to open the drawer.
      </p>
      <br><br>

      <h3 class="title is-5 ">MetaWorld - Drawer Open - Scenario II</h3>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/drawer_open_2.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <p>
        In the second scenario in the drawer open task,
        all agents open the drawer, but the agents trained using pairwise and sequential pairwise comparison are unstable because their end effectors oscillate with a large and small amplitude, respectively.
      </p>
      <br><br>

      <h3 class="title is-5 ">MetaWorld - Window Open</h3>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/window_open.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <p>
        In the example trajectories in the window open task,
        only the agent trained using root pairwise comparison succeeds in opening the window.
      </p>
      <br><br>

      <h3 class="title is-5 ">MetaWorld - Hammer</h3>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/hammer.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <p>
        In the example trajectories in the hammer task,
        agents trained using sequential and root pairwise comparison succeed in driving a nail into the wooden box.
      </p>
      <br><br>

      <h3 class="title is-4 ">Real Human Feedback Experiments</h3>
      <!-- <h3 class="title is-5 ">DMControl - Cheetah Run</h3> -->
      <p>
        We conduct experiments with real human feedback to compare the user stress level for each method.
Each participant trained a cheetah to run as fast as it can.
      </p>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/cheetah_pairwise.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <p>
        Pairwise Comparison (Baseline)
      </p>
      <br><br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/cheetah_pairwise.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <p>
        Sequential Pairwise Comparison (Ours)
      </p>
      <br><br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/cheetah_pairwise.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <p>
        Root Pairwise Comparison (Ours)
      </p>
      <br><br>

      <!-- <p>
        Root pairwise comparison is the least burdensome for real human users while achieving the highest performance.
      </p> -->
      <h3 class="title is-5 ">Root pairwise comparison is the least burdensome for real human users while achieving the highest performance.</h3>

      <p>
        After the experiments end, the participants took a survey.
        Participants responed that the user satisfaction scores are 2.20 (pairwise), 3.00 (sequential), and 3.93 (root).
        The most significant preference criterion was the overall moved distance of the agent.
      </p>
      <br>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/images/real_human_feedback_plot.png" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <br><br>

      <h3 class="title is-4 ">Real Robot Experiments - Block Placing</h3>
      <p>
        To demonstrate our method in real-world environments, we conduct a block placing task using a real UR-5 robot.
      </p>

      <div class="columns is-vcentered is-centered">
        <br>
        <img src="static/videos/real_block_placing.gif" class="interpolation-image" alt="Interpolate start reference image."
         style="width: 90%;" />
      </div>

      <!-- <p>
        To demonstrate our method in real-world environments, we conduct a block placing task using a real UR-5 robot.
      </p> -->
      <br><br>

    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{hwang2023seqrank,
     author    = {Hwang, Minyoung and Lee, Gunmin and Kee, Hogun and Kim, Chan Woo and Lee, Kyungjae and Oh, Songhwai},
     title     = {Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback},
     booktitle = {Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS)},
     month     = {December},
     year      = {2023},
 }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/2023_CVPR_arXiv.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/obin-hero" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a rel="Nerfies" href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
